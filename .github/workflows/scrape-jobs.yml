name: Scrape jobs (daily 6am & 3pm ET)

on:
  schedule:
    - cron: "7 * * * *"    # run hourly; we gate to 06:00/15:00 ET in-job (handles DST)
  workflow_dispatch: {}

permissions:
  contents: write          # allow git push

jobs:
  scrape:
    runs-on: ubuntu-latest
    env:
      TZ: America/New_York

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Ensure timezone is ET on runner
        uses: szenius/set-timezone@v2.0
        with:
          timezoneLinux: "America/New_York"

      - name: Decide whether to run now (only at 06:00 or 15:00 ET)
        id: gate
        shell: bash
        run: |
          set -eu
          HOUR=$(date +%H)
          MIN=$(date +%M)
          echo "Local ET time is $HOUR:$MIN"
          if [[ "$HOUR" == "06" || "$HOUR" == "15" ]]; then
            echo "run_scrape=true" >> "$GITHUB_OUTPUT"
          else
            echo "run_scrape=false" >> "$GITHUB_OUTPUT"

      - name: Diagnostics
        if: steps.gate.outputs.run_scrape == 'true'
        run: |
          echo "Working dir: $(pwd)"
          ls -la

      - name: Install sqlite3
        if: steps.gate.outputs.run_scrape == 'true'
        run: |
          sudo apt-get update
          sudo apt-get install -y sqlite3

      - name: Setup Python
        if: steps.gate.outputs.run_scrape == 'true'
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Python deps
        if: steps.gate.outputs.run_scrape == 'true'
        run: |
          set -eux
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 playwright pandas tenacity
          python -m playwright install --with-deps chromium

      - name: Run scraper
        if: steps.gate.outputs.run_scrape == 'true'
        shell: bash
        run: |
          set -euxo pipefail
          # ðŸ”§ Edit these to your target companies & sites:
          python jobscraper.py \
            --greenhouse stripe snowflake \
            --lever databricks \
            --smartrecruiters nvidia \
            --locations "United States" Remote "Washington, DC" \
            --include-titles data engineer scientist analyst sde software ml ai \
            --include-keywords python sql pyspark airflow dbt spark aws gcp

      - name: Ensure DB/table exist
        if: steps.gate.outputs.run_scrape == 'true'
        run: |
          set -eux
          python - <<'PY'
from jobscraper import init_db
init_db()
print("DB ok.")
PY

      - name: Export CSV (header if empty)
        if: steps.gate.outputs.run_scrape == 'true'
        run: |
          set -eux
          if [ ! -f jobs.sqlite ]; then
            printf "posted,company,title,location,department,url\n" > jobs.csv
          else
            ROWS=$(sqlite3 jobs.sqlite "select count(*) from jobs;" || echo "0")
            if [ "${ROWS:-0}" = "0" ]; then
              printf "posted,company,title,location,department,url\n" > jobs.csv
            else
              sqlite3 jobs.sqlite -header -csv \
                "select coalesce(date(posted_at), '') as posted, company, title, location, department, url from jobs order by posted desc" \
                > jobs.csv
            fi
          fi
          wc -l jobs.csv || true

      - name: Upload CSV artifact
        if: steps.gate.outputs.run_scrape == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: jobs-latest
          path: jobs.csv
          if-no-files-found: warn
          retention-days: 7

      - name: Commit CSV & DB to repo
        if: steps.gate.outputs.run_scrape == 'true'
        run: |
          set -eux
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add jobs.sqlite jobs.csv || true
          git commit -m "Auto-update jobs ($(date -u +'%Y-%m-%d %H:%M UTC'))" || echo "No changes"
          git push || true
